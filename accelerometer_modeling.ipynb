{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Activity detection with accelerometer data\n",
        "\n",
        "In this notebook, we will compare the performance of an activity detection model trained with and without using featurization with Fast Fourier Transform\n",
        "\n",
        "This notebook must be run in a Synapse Analytics workspace in order to work as expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "container_name = \"YOUR_STORAGE_CONTAINER\"\n",
        "account_name = \"YOUR_STORAGE_ACCOUNT\"\n",
        "\n",
        "workspace = mssparkutils.runtime.context[\"workspace\"]\n",
        "mssparkutils.fs.mount( \n",
        "    f\"abfss://{container_name}@{account_name}.dfs.core.windows.net\", \n",
        "    \"/data\",\n",
        "    {\"linkedService\":f\"{workspace}-WorkspaceDefaultStorage\"} \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00287/Activity%20Recognition%20from%20Single%20Chest-Mounted%20Accelerometer.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PASTE THE JOB ID PRINTED BELOW IN THE NEXT CELL\n",
        "mssparkutils.env.getJobId()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!unzip -d /synfs/JOB_ID_HERE/data/ \"Activity Recognition from Single Chest-Mounted Accelerometer.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm \"Activity Recognition from Single Chest-Mounted Accelerometer.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 0. Libraries\n",
        "We'll begin by importing all the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pip install more_spark_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pyspark import keyword_only\n",
        "from pyspark.sql import Window, SparkSession\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import lit, to_timestamp, lag, window, mean, stddev, max, min, skewness, kurtosis, col, rand, last\n",
        "from pyspark.ml import Transformer, Pipeline, PipelineModel, Estimator\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.param.shared import HasInputCols, HasOutputCol, Param, Params, TypeConverters\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from more_spark_transformers import FFTTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"FFT with IoT data sample\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Parameterization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Numerical input columns to be analyzed\n",
        "INPUT_COLUMNS = ['AccelerationX', 'AccelerationY', 'AccelerationZ']\n",
        "# Multiclass label column. Should be of type int\n",
        "OUTPUT_COLUMN = 'Label'\n",
        "# Column to partition data before training. Sensor identifier, etc. Should be of type int\n",
        "PARTITION_COLUMN = 'ParticipantId'\n",
        "\n",
        "# If data does not contain an explicit timestamp column, provide the measure index and time between samples. \n",
        "# Sampling rate should be constant.\n",
        "SAMPLING_COLUMN = 'SequentialNumber'\n",
        "SAMPLING_PERIOD = 1 / 52\n",
        "TIMESTAMP_COLUMN = 'Timestamp' # If not present, will be created based on the above\n",
        "\n",
        "# FFT transformation parameters. Window duration should be as small as possible while allowing a prediction - imagine if a human were to make the prediction, how much time would they need observing the inputs? Window slide duration defines how much overlap to leave between data points - more overlap means more training data, but also more risk of overfitting the model.\n",
        "FFT_WINDOW_DURATION = 8 / 52\n",
        "FFT_WINDOW_SLIDE_DURATION = 8 / 52\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "FFT_WINDOW_SIZE = round(1 / SAMPLING_PERIOD * FFT_WINDOW_DURATION) # Calculated based on the above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1. Importing data to Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "acceleration_data_raw = spark.read.csv(f\"synfs:/{mssparkutils.env.getJobId()}/data/Activity Recognition from Single Chest-Mounted Accelerometer\")\n",
        "\n",
        "acceleration_data_raw = acceleration_data_raw.withColumnRenamed(\"_c0\", \"SequentialNumber\")\n",
        "acceleration_data_raw = acceleration_data_raw.withColumnRenamed(\"_c1\", \"AccelerationX\")\n",
        "acceleration_data_raw = acceleration_data_raw.withColumnRenamed(\"_c2\", \"AccelerationY\")\n",
        "acceleration_data_raw = acceleration_data_raw.withColumnRenamed(\"_c3\", \"AccelerationZ\")\n",
        "acceleration_data_raw = acceleration_data_raw.withColumnRenamed(\"_c4\", \"Label\")\n",
        "acceleration_data_raw = acceleration_data_raw.withColumn(\"ParticipantId\", F.udf(lambda x: x.split('/')[-1][:-4])(F.input_file_name()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "acceleration_data_raw.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def format_data(\n",
        "    raw_data, \n",
        "    inputCols=[],\n",
        "    outputCol=None,\n",
        "    partitionCol=None,\n",
        "    samplingCol=None,\n",
        "    samplingPeriod=None,\n",
        "    timestampCol=None,\n",
        "):\n",
        "    data = raw_data\n",
        "\n",
        "    for input_column in inputCols:\n",
        "        data = data.withColumn(input_column, col(input_column).cast(\"int\"))\n",
        "        \n",
        "    data = data.withColumn(partitionCol, col(partitionCol))\n",
        "    data = data.withColumn(samplingCol, col(samplingCol).cast(\"int\"))\n",
        "    data = data.withColumn(outputCol, col(outputCol).cast(\"int\"))\n",
        "\n",
        "    if samplingCol and samplingPeriod:\n",
        "        data = data.withColumn(timestampCol, to_timestamp(samplingPeriod * col(samplingCol)))\n",
        "\n",
        "    data = data.select(timestampCol, partitionCol, *inputCols, outputCol)\n",
        "    data = data.orderBy(partitionCol, timestampCol)\n",
        "    return data\n",
        "\n",
        "acceleration_data = format_data(\n",
        "    acceleration_data_raw,\n",
        "    inputCols=INPUT_COLUMNS,\n",
        "    outputCol=OUTPUT_COLUMN,\n",
        "    partitionCol=PARTITION_COLUMN,\n",
        "    samplingCol=SAMPLING_COLUMN,\n",
        "    samplingPeriod=SAMPLING_PERIOD,\n",
        "    timestampCol=TIMESTAMP_COLUMN,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2. Exploratory data analysis\n",
        "\n",
        "Based on the documentation in the README and what we see in the data,\n",
        "we have the following information:\n",
        "\n",
        "- **ParticipantId**: participant identifier.\n",
        "- **SequentialNumber**: incremental column for each participant in the study, \n",
        "indicating the point in time the measure was taken. Each increment is\n",
        "equivalent to 1/52nd of a second.\n",
        "- **AccelerationX, AccelerationY and AccelerationZ**: accelerometer readings with regards to the three dimensions\n",
        "of space. The accelerometer is chest mounted, which means the axes are\n",
        "constantly aligned with the participant's body.\n",
        "- **Label**: activity label index, from 1-7, indicating seven different\n",
        "activities we would like to predict, based on movement.\n",
        "\n",
        "As we are to predict a **categorical value** (activity), we will treat\n",
        "this as a **classification problem**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "One of the first activities in Exploratory Data Analysis is to understand whether the input columns correlate\n",
        "with the output in some way. This can be done in many ways, including visually.\n",
        "Next, as an example let's look at how AccelerationZ (as an example) correlates to the activity being performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "target_df = acceleration_data.where('ParticipantId = 1').toPandas()\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Line(x=target_df['Timestamp'], y=(target_df['AccelerationZ']-target_df['AccelerationZ'].mean()+11)/target_df['AccelerationZ'].std()),\n",
        "    go.Line(x=target_df['Timestamp'], y=target_df['Label'])\n",
        "])\n",
        "fig.update_yaxes(\n",
        "    range=[-3,8],  # sets the range of xaxis\n",
        "    constrain=\"domain\",  # meanwhile compresses the xaxis by decreasing its \"domain\"\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Notice how the patterns are significantly different for different activities:\n",
        "- Activity 1 (working at a computer): Long periods with low activity;\n",
        "- Activity 4 (walking): Constant activity;\n",
        "\n",
        "Also notice how an instantaneous acceleration reading is probably not enough to determine the current activity.\n",
        "We will need to perform some kind of window aggregation to find meaning in our inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Now, turning our attention to the labels, we can plot the distribution of activities to acess whether the dataset\n",
        "is imbalanced, which would require additional precautions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "activity_counts = acceleration_data.groupBy(OUTPUT_COLUMN).agg({'*': 'count'}).toPandas()\n",
        "fig = px.bar(activity_counts, x=OUTPUT_COLUMN, y='count(1)')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "A few problems are identified, and the following additional steps are taken:\n",
        "\n",
        "- An unidentified label \"0\" is present in the dataset. We will drop these rows as we cannot interpret what they mean.\n",
        "- Labels 2, 5, and 6 are very low frequency (under 10% of the dataset) and likely similar to other labels.\n",
        "We propose eliminating label 2, as it's in fact a combination of multiple other actions in sequence, and joining\n",
        "labels 4 and 6, as both mean the participant is walking. As label 5 refers to going up/down stairs, we expect\n",
        "it will be easily identifiable, so we will not alter it right now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "acceleration_data_clean = acceleration_data\\\n",
        "    .where(col(OUTPUT_COLUMN) != 0)\\\n",
        "    .where(col(OUTPUT_COLUMN) != 2)\\\n",
        "    .withColumn(OUTPUT_COLUMN, when(acceleration_data.__getattr__(OUTPUT_COLUMN) == 6, 4).otherwise(acceleration_data.__getattr__(OUTPUT_COLUMN)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3. Modeling\n",
        "\n",
        "We'll train and evaluate two models using the exact same data:\n",
        "\n",
        "- On model 1, we will use the data as-is, without feature engineering;\n",
        "- On model 2, we will use FFT decomposition before modeling.\n",
        "\n",
        "Check out the blog (linked in the README) for details on what the FFT transform is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_data = acceleration_data_clean.where(acceleration_data_clean.ParticipantId != \"1\")\n",
        "test_data = acceleration_data_clean.where(acceleration_data_clean.ParticipantId == \"1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# At this point, we're using an imported FFTTransformer published to pip.\n",
        "# This is to ensure that pickling and sharing models is a smoother experience.\n",
        "# In case you're wondering what the source code for the transformer looks like, it is copied below\n",
        "\n",
        "# import numpy as np\n",
        "# import pyspark.sql.types as T\n",
        "# import pyspark.sql.functions as F\n",
        "# from pyspark import keyword_only\n",
        "# from pyspark.sql.functions import window, col, rand, last\n",
        "# from pyspark.ml import Transformer\n",
        "# from pyspark.ml.param.shared import HasInputCols, HasOutputCol, Param, Params, TypeConverters\n",
        "# from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
        "\n",
        "\n",
        "# class FFTTransformer(Transformer, HasInputCols, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
        "#     inputCols = Param(Params._dummy(), \"inputCols\", \"inputCols\", typeConverter=TypeConverters.toListString)\n",
        "#     windowDuration = Param(Params._dummy(), \"windowDuration\", \"windowDuration\", typeConverter=TypeConverters.toFloat)\n",
        "#     windowSlideDuration = Param(Params._dummy(), \"windowSlideDuration\", \"windowSlideDuration\", typeConverter=TypeConverters.toFloat)\n",
        "#     samplingPeriod = Param(Params._dummy(), \"samplingPeriod\", \"samplingPeriod\", typeConverter=TypeConverters.toFloat)\n",
        "#     partitionColumn = Param(Params._dummy(), \"partitionColumn\", \"partitionColumn\", typeConverter=TypeConverters.toString)\n",
        "#     timestampColumn = Param(Params._dummy(), \"timestampColumn\", \"timestampColumn\", typeConverter=TypeConverters.toString)\n",
        "#     outputCol = Param(Params._dummy(), \"outputCol\", \"outputCol\", typeConverter=TypeConverters.toString)\n",
        "\n",
        "#     @keyword_only\n",
        "#     def __init__(self,inputCols=[], windowDuration=None, windowSlideDuration=None, samplingPeriod=None, partitionColumn=None, timestampColumn=None, outputCol=None):\n",
        "#         super(FFTTransformer, self).__init__()\n",
        "#         kwargs = self._input_kwargs\n",
        "#         self.set_params(**kwargs)\n",
        "#         self.uid = 'FFTTransformer'\n",
        "#         self.fft_udf = F.udf(lambda z: [float(i) for i in np.fft.fft(z)], T.ArrayType(T.FloatType()))\n",
        "\n",
        "  \n",
        "#     @keyword_only\n",
        "#     def set_params(self, **kwargs):\n",
        "#         self._set(**kwargs)\n",
        "    \n",
        "\n",
        "#     def get_fft(self, column):\n",
        "#         return self.fft_udf(F.collect_list(col(column))).alias(f\"{column}_FFT\")\n",
        "\n",
        "\n",
        "#     def _transform(self, dataset):\n",
        "\n",
        "#         inputCols = self.getOrDefault(\"inputCols\")\n",
        "#         windowDuration = self.getOrDefault(\"windowDuration\")\n",
        "#         windowSlideDuration = self.getOrDefault(\"windowSlideDuration\")\n",
        "#         samplingPeriod = self.getOrDefault(\"samplingPeriod\")\n",
        "#         partitionColumn = self.getOrDefault(\"partitionColumn\")\n",
        "#         timestampColumn = self.getOrDefault(\"timestampColumn\")\n",
        "#         outputCol = self.getOrDefault(\"outputCol\")\n",
        "\n",
        "#         windowDurationString = f'{round(1000 * windowDuration)} milliseconds'\n",
        "#         windowSlideDurationString = f'{round(1000 * windowSlideDuration)} milliseconds'\n",
        "#         windowSize = round(1 / samplingPeriod * windowDuration)\n",
        "\n",
        "#         fft_columns = [self.get_fft(column) for column in inputCols]\n",
        "#         fft_column_names = [f\"{column}_FFT\" for column in inputCols ]\n",
        "\n",
        "#         df =  dataset\\\n",
        "#             .groupBy(partitionColumn, window(timestampColumn, windowDuration=windowDurationString, slideDuration=windowSlideDurationString))\\\n",
        "#             .agg(\n",
        "#                 *fft_columns,\n",
        "#                 last(col(outputCol))\n",
        "#             )\n",
        "\n",
        "#         fft_column_components = [df[f\"{column}_FFT\"][index] for column in inputCols for index in range(windowSize) ]\n",
        "\n",
        "#         return df\\\n",
        "#             .select('*', *fft_column_components)\\\n",
        "#             .drop(*fft_column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "fftTransformer = FFTTransformer(\n",
        "    inputCols=INPUT_COLUMNS,\n",
        "    windowDuration=FFT_WINDOW_DURATION,\n",
        "    windowSlideDuration=FFT_WINDOW_SLIDE_DURATION,\n",
        "    samplingPeriod=SAMPLING_PERIOD,\n",
        "    partitionColumn=PARTITION_COLUMN,\n",
        "    timestampColumn=TIMESTAMP_COLUMN,\n",
        "    outputCol=OUTPUT_COLUMN\n",
        ")\n",
        "FFT_COLUMNS = [f\"{column}_FFT[{index}]\" for column in INPUT_COLUMNS for index in range(FFT_WINDOW_SIZE)]\n",
        "vectorAssembler_1 = VectorAssembler(inputCols=INPUT_COLUMNS, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "vectorAssembler_2 = VectorAssembler(inputCols=FFT_COLUMNS, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "scaler_1 = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
        "scaler_2 = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
        "rf_1 = RandomForestClassifier(numTrees=100, featuresCol=\"features_scaled\", labelCol=OUTPUT_COLUMN)\n",
        "rf_2 = RandomForestClassifier(numTrees=100, featuresCol=\"features_scaled\", labelCol=f\"last({OUTPUT_COLUMN})\")\n",
        "\n",
        "\n",
        "pipeline_1 = Pipeline(stages=[\n",
        "    vectorAssembler_1, \n",
        "    scaler_1,\n",
        "    rf_1\n",
        "])\n",
        "pipeline_2 = Pipeline(stages=[\n",
        "    fftTransformer,\n",
        "    vectorAssembler_2, \n",
        "    scaler_2,\n",
        "    rf_2\n",
        "])\n",
        "\n",
        "finalModel_1 = pipeline_1.fit(training_data)\n",
        "finalModel_2 = pipeline_2.fit(training_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 4. Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "predictions_1 = finalModel_1.transform(test_data)\n",
        "preds_and_labels_1 = predictions_1.select(['prediction','Label']).withColumn('label', col('Label').cast('float')).orderBy('prediction')\n",
        "preds_and_labels_1 = preds_and_labels_1.select(['prediction','label'])\n",
        "\n",
        "predictions_2 = finalModel_2.transform(test_data)\n",
        "preds_and_labels_2 = predictions_2.select(['prediction','last(Label)']).withColumn('label', col('last(Label)').cast('float')).orderBy('prediction')\n",
        "preds_and_labels_2 = preds_and_labels_2.select(['prediction','label'])\n",
        "\n",
        "metrics_1= MulticlassMetrics(preds_and_labels_1.rdd.map(tuple))\n",
        "metrics_2= MulticlassMetrics(preds_and_labels_2.rdd.map(tuple))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(f\"Regular data model accuracy: {metrics_1.accuracy}\")\n",
        "\n",
        "print(f\"FFT data model accuracy: {metrics_2.accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "58964be474762df96cfbf445054a39a59e033e1d46085eace5af65b193473df7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
